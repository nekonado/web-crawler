name: Scheduled Web Crawler with Commit and Gist Upload

on:
  schedule:
    # 日本時間で月曜日の午前3時（UTC日曜日18:00）
    - cron: "0 18 * * 0"
  workflow_dispatch: # 手動実行も可能にする

permissions:
  contents: write

jobs:
  crawl-and-commit:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.9"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Copy sample config if needed
        run: |
          if [ ! -f config.json ]; then
            cp config.sample.json config.json
          fi

      - name: Run crawler
        run: |
          mkdir -p output/actions
          OUTPUT_DIR=output/actions python crawler.py

      - name: Process crawler results
        id: process-results
        run: |
          # 日付付きのCSVファイルを見つけてoutput/actionsにコピー
          LATEST_CSV=$(ls -t output/crawl_result_*.csv | head -1)
          cp "${LATEST_CSV}" output/actions/
          echo "latest_csv=${LATEST_CSV}" >> $GITHUB_OUTPUT

          # latestバージョンも作成（Gist用）
          cp "${LATEST_CSV}" output/crawl_result_latest.csv

      - name: Commit results
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add output/actions/
          git add output/crawl_result_latest.csv
          git commit -m "Update crawler results [skip ci]" || echo "No changes to commit"
          git push

      - name: Upload to GitHub Gist
        uses: exuanbo/actions-deploy-gist@v1
        with:
          token: ${{ secrets.GIST_TOKEN }}
          gist_id: ${{ secrets.GIST_ID }}
          file_path: output/crawl_result_latest.csv
          file_type: text
